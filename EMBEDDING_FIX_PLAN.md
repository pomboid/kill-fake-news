# Plano de Corre√ß√£o: Embeddings e Failover Multi-Provider

## üî¥ Problemas Identificados

### 1. Rate Limit do Gemini (BLOQUEADO)
- **Status**: Gemini est√° bloqueado por exceder TPM (36.03K/30K)
- **Modelo atual**: `models/embedding-001` retorna 404 (modelo incorreto ou deprecado)
- **Impacto**: N√£o pode gerar embeddings mesmo com modelo correto at√© resetar o rate limit

### 2. Incompatibilidade de Dimensionalidade (CR√çTICO)
```python
# sql_models.py linha 58
embedding: Optional[List[float]] = Field(default=None, sa_column=Column(Vector(768)))
```

**Dimens√µes dos providers dispon√≠veis:**
- ‚ùå Gemini: 768 dims (COMPAT√çVEL, mas bloqueado por rate limit)
- ‚ùå OpenAI: 1536 dims (INCOMPAT√çVEL - falhar√° ao inserir no banco)
- ‚ùå Cohere: 1024 dims (INCOMPAT√çVEL - falhar√° ao inserir no banco)
- ‚ùå Together: 1024 dims (INCOMPAT√çVEL - falhar√° ao inserir no banco)
- ‚ùå Groq: N√£o tem embeddings
- ‚ùå DeepSeek: N√£o verificado

### 3. Failover N√£o Funcional
- O failover est√° implementado no c√≥digo mas **bloqueado pela restri√ß√£o de dimensionalidade**
- Mesmo que tente OpenAI/Cohere/Together, o PostgreSQL rejeitar√° vetores com dimens√µes diferentes de 768
- Logs mostram apenas tentativas do Gemini (10x) - outros providers n√£o foram tentados

---

## ‚úÖ SOLU√á√ïES PROPOSTAS

### üéØ **OP√á√ÉO 1: Migrar para OpenAI (1536 dims) [RECOMENDADO]**

**Vantagens:**
- ‚úÖ OpenAI √© o mais est√°vel e confi√°vel
- ‚úÖ Voc√™ j√° tem a API key configurada
- ‚úÖ 1536 dims = melhor qualidade de embeddings
- ‚úÖ Suporte a failover para Cohere/Together (com truncation)

**Passos de implementa√ß√£o:**

1. **Criar migra√ß√£o do banco de dados**
   ```sql
   -- Arquivo: migrations/002_upgrade_embedding_dimensions.sql

   -- 1. Criar nova coluna tempor√°ria com 1536 dims
   ALTER TABLE article ADD COLUMN embedding_new vector(1536);

   -- 2. Migrar embeddings existentes (padding com zeros)
   UPDATE article
   SET embedding_new = array_cat(embedding::float[], array_fill(0.0::float, ARRAY[768])::float[])
   WHERE embedding IS NOT NULL;

   -- 3. Remover coluna antiga e renomear
   ALTER TABLE article DROP COLUMN embedding;
   ALTER TABLE article RENAME COLUMN embedding_new TO embedding;

   -- 4. Recriar √≠ndice para busca vetorial
   CREATE INDEX ON article USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);
   ```

2. **Atualizar sql_models.py**
   ```python
   # Linha 58
   embedding: Optional[List[float]] = Field(default=None, sa_column=Column(Vector(1536)))
   ```

3. **Implementar adaptador de dimensionalidade**
   ```python
   # core/llm/embedding_adapter.py (NOVO)

   def adapt_embedding(embedding: List[float], target_dims: int = 1536) -> List[float]:
       """Adapta embedding para dimensionalidade alvo"""
       current_dims = len(embedding)

       if current_dims == target_dims:
           return embedding
       elif current_dims < target_dims:
           # Padding com zeros
           return embedding + [0.0] * (target_dims - current_dims)
       else:
           # Truncation (primeiros N valores)
           return embedding[:target_dims]
   ```

4. **Atualizar manager.py para usar adaptador**
   ```python
   # Linha 231 - get_embedding()
   result = await provider.get_embedding(text, model)
   result = adapt_embedding(result, target_dims=1536)  # Normalizar para 1536
   return result
   ```

5. **Re-indexar artigos existentes**
   ```bash
   # Limpar embeddings antigos e re-gerar com OpenAI
   docker compose exec backend python -c "
   from core.database import get_session
   from core.sql_models import Article
   from sqlmodel import select
   import asyncio

   async def clear_embeddings():
       async for session in get_session():
           stmt = select(Article)
           result = await session.execute(stmt)
           articles = result.scalars().all()
           for article in articles:
               article.embedding = None
           await session.commit()

   asyncio.run(clear_embeddings())
   print('Embeddings cleared. Run: python main.py index')
   "

   # Re-indexar com OpenAI
   docker compose exec backend python main.py index
   ```

**Custo estimado:** ~$0.10 por 1M tokens (muito barato)

**Tempo de implementa√ß√£o:** 2-3 horas
**Risco:** M√©dio (requer migra√ß√£o de banco)

---

### üéØ **OP√á√ÉO 2: Corrigir Gemini + Provider Local de Backup**

**Vantagens:**
- ‚úÖ Mant√©m 768 dims (sem migra√ß√£o de banco)
- ‚úÖ Usa Gemini quando dispon√≠vel (GRATUITO)
- ‚úÖ Fallback local quando Gemini est√° bloqueado
- ‚úÖ Sem custos de API

**Passos de implementa√ß√£o:**

1. **Descobrir modelo correto do Gemini**
   Testar modelos poss√≠veis:
   - `text-multilingual-embedding-002` (768 dims)
   - `text-embedding-005` (768 dims)
   - `embedding-001` (sem prefixo "models/")

   ```python
   # gemini_provider.py linha 49
   @property
   def default_embedding_model(self) -> str:
       return "text-multilingual-embedding-002"  # Testar este
   ```

2. **Implementar provider local com SentenceTransformers**
   ```python
   # core/llm/providers/local_provider.py (NOVO)

   from sentence_transformers import SentenceTransformer
   from ..base import EmbeddingProvider

   class LocalEmbeddingProvider(EmbeddingProvider):
       """Provider local usando SentenceTransformers (768 dims)"""

       def __init__(self, api_key: Optional[str] = None):
           super().__init__(api_key="local")  # Sempre dispon√≠vel
           self.model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')

       @property
       def embedding_dimensions(self) -> int:
           return 768

       async def get_embedding(self, text: str, model: Optional[str] = None) -> List[float]:
           try:
               # Modelo local n√£o precisa de API key
               embedding = self.model.encode(text[:8000], convert_to_numpy=True)
               self.mark_success()
               return embedding.tolist()
           except Exception as e:
               self.mark_failure(e)
               raise
   ```

3. **Adicionar LocalProvider ao manager**
   ```python
   # manager.py linha 57-69
   provider_classes = [
       (GroqProvider, 'groq'),
       (GeminiProvider, 'gemini'),
       (OpenAIProvider, 'openai'),
       (AnthropicProvider, 'anthropic'),
       (DeepSeekProvider, 'deepseek'),
       (MistralProvider, 'mistral'),
       (TogetherProvider, 'together'),
       (CohereProvider, 'cohere'),
       (LocalEmbeddingProvider, 'local'),  # SEMPRE DISPON√çVEL (n√£o precisa API key)
   ]
   ```

4. **Atualizar requirements.txt**
   ```
   sentence-transformers==2.3.1
   torch==2.1.2
   ```

5. **Rebuild do container**
   ```bash
   docker compose down
   docker compose build backend
   docker compose up -d
   docker compose exec backend python main.py index
   ```

**Custo:** GRATUITO (modelo local)
**Tempo de implementa√ß√£o:** 1-2 horas
**Risco:** Baixo (sem migra√ß√£o de banco)
**Desvantagem:** Embeddings locais podem ser de qualidade inferior

---

### üéØ **OP√á√ÉO 3: Sistema H√≠brido (Melhor dos Dois Mundos)**

**Vantagens:**
- ‚úÖ Usa OpenAI quando dispon√≠vel (melhor qualidade)
- ‚úÖ Fallback para modelo local (sempre dispon√≠vel)
- ‚úÖ Suporta m√∫ltiplas dimensionalidades com adaptador

**Implementa√ß√£o:** Combina Op√ß√£o 1 + Op√ß√£o 2
1. Migrar banco para 1536 dims
2. Implementar adaptador de dimensionalidade
3. Adicionar LocalProvider como √∫ltimo fallback
4. Ordem de prioridade: OpenAI ‚Üí Cohere ‚Üí Together ‚Üí Gemini ‚Üí Local

**Custo:** ~$0.10 por 1M tokens (muito barato, s√≥ quando API √© usada)
**Tempo de implementa√ß√£o:** 3-4 horas
**Risco:** M√©dio-Alto (mais complexo)

---

## üéØ RECOMENDA√á√ÉO FINAL

### Para Implementar AGORA (Curto Prazo):
**Escolha OP√á√ÉO 2** - Corrigir Gemini + Provider Local

**Raz√µes:**
1. ‚úÖ **Mais r√°pido** (1-2 horas vs 2-4 horas)
2. ‚úÖ **Menor risco** (sem migra√ß√£o de banco)
3. ‚úÖ **GRATUITO** (modelo local sem custo de API)
4. ‚úÖ **Sempre dispon√≠vel** (n√£o depende de rate limits)
5. ‚úÖ **N√£o perde dados** (mant√©m embeddings existentes com 768 dims)

### Para Implementar DEPOIS (Longo Prazo):
**Migrar para OP√á√ÉO 1** quando o projeto crescer

**Raz√µes:**
1. OpenAI tem melhor qualidade de embeddings
2. 1536 dims = busca sem√¢ntica mais precisa
3. Mais estabilidade (SLA 99.9%)
4. Custo muito baixo ($0.10 por 1M tokens)

---

## üöÄ PR√ìXIMOS PASSOS

**Aguardando decis√£o do usu√°rio:**
1. Qual op√ß√£o implementar? (Op√ß√£o 1, 2 ou 3)
2. Ap√≥s escolha, come√ßar implementa√ß√£o imediatamente
3. Testar em ambiente de desenvolvimento primeiro
4. Deploy em produ√ß√£o ap√≥s valida√ß√£o

---

## üìä COMPARA√á√ÉO R√ÅPIDA

| Crit√©rio | Op√ß√£o 1 (OpenAI) | Op√ß√£o 2 (Gemini+Local) | Op√ß√£o 3 (H√≠brido) |
|----------|------------------|------------------------|-------------------|
| Custo | ~$0.10/1M tokens | GRATUITO | ~$0.10/1M tokens |
| Tempo | 2-3h | 1-2h | 3-4h |
| Risco | M√©dio | Baixo | M√©dio-Alto |
| Qualidade | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Disponibilidade | 99.9% | 100% | 100% |
| Migra√ß√£o DB | ‚úÖ Sim | ‚ùå N√£o | ‚úÖ Sim |

**Recomenda√ß√£o:** OP√á√ÉO 2 agora ‚Üí OP√á√ÉO 1 depois
