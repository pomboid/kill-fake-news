# Multi-Provider LLM System

Sistema de gerenciamento de m√∫ltiplos provedores de IA com failover autom√°tico e load balancing.

## üéØ Funcionalidades

- ‚úÖ **Failover Autom√°tico**: Se um provedor falhar, automaticamente tenta o pr√≥ximo
- ‚úÖ **Load Balancing**: Distribui requisi√ß√µes entre provedores (round-robin)
- ‚úÖ **Prioriza√ß√£o**: Provedores gratuitos s√£o priorizados
- ‚úÖ **Health Tracking**: Desabilita provedores ap√≥s 3 falhas consecutivas
- ‚úÖ **Embeddings Multi-Provider**: Suporte a embeddings de m√∫ltiplos provedores

## ü§ñ Provedores Suportados

### Gratuitos (Prioridade M√°xima)
1. **Groq** - FREE, ilimitado
   - Llama 3.3 70B, Gemma 2 9B, Mixtral 8x7B
   - Velocidade: Ultra-r√°pida

2. **Gemini** - FREE tier generoso
   - Gemini 2.0 Flash, Gemini 1.5 Pro
   - Embeddings: 768 dimens√µes

### Freemium (Fallback)
3. **OpenAI** - $0.15/1M tokens (gpt-4o-mini)
   - GPT-4o, GPT-4o-mini
   - Embeddings: 1536 dimens√µes

4. **Anthropic** - $0.25/1M tokens (Haiku)
   - Claude 3.5 Sonnet, Claude 3.5 Haiku
   - Melhor para an√°lise textual

## üìù Uso

### Configura√ß√£o B√°sica

```python
from core.llm import LLMManager

# Inicializar manager
manager = LLMManager(
    enabled_providers=['groq', 'gemini', 'openai'],
    api_keys={
        'groq': 'your_groq_key',
        'gemini': 'your_gemini_key',
        'openai': 'your_openai_key'
    },
    load_balance=False  # False = failover only, True = round-robin
)
```

### Gerar Texto

```python
# Gera texto com failover autom√°tico
text = await manager.generate_text(
    prompt="Explique o que √© fake news",
    temperature=0.7
)
```

### Gerar JSON

```python
# Gera JSON estruturado
data = await manager.generate_json(
    prompt="Analise este artigo e retorne JSON: {...}",
    temperature=0.5
)
```

### Gerar Embeddings

```python
# Gera embedding com failover
embedding = await manager.get_embedding("texto para vetorizar")
```

### Verificar Status

```python
# Ver status de todos os provedores
status = manager.get_status()
print(status)
```

## üîß Vari√°veis de Ambiente

```env
# API Keys
GROQ_API_KEY=your_key
GEMINI_API_KEY=your_key
OPENAI_API_KEY=your_key
ANTHROPIC_API_KEY=your_key

# Configura√ß√£o
ENABLED_PROVIDERS=groq,gemini,openai,anthropic
LOAD_BALANCE=false
```

## üèóÔ∏è Arquitetura

```
core/llm/
‚îú‚îÄ‚îÄ base.py                    # Classes abstratas
‚îú‚îÄ‚îÄ manager.py                 # Gerenciador principal
‚îî‚îÄ‚îÄ providers/
    ‚îú‚îÄ‚îÄ groq_provider.py       # Groq (FREE)
    ‚îú‚îÄ‚îÄ gemini_provider.py     # Gemini (FREE + embeddings)
    ‚îú‚îÄ‚îÄ openai_provider.py     # OpenAI (embeddings)
    ‚îî‚îÄ‚îÄ anthropic_provider.py  # Claude
```

## üé® Exemplo Completo

```python
from core.llm import LLMManager
from core.config import Config

# Criar manager usando config
manager = LLMManager(
    enabled_providers=Config.ENABLED_PROVIDERS,
    api_keys=Config.get_provider_api_keys(),
    load_balance=Config.LOAD_BALANCE
)

# An√°lise de not√≠cia com failover
try:
    analysis = await manager.generate_json(
        prompt=f"Analise esta not√≠cia: {article.content}"
    )
    print(f"Resultado: {analysis}")
except Exception as e:
    print(f"Todos os provedores falharam: {e}")

# Verificar qual provedor foi usado
status = manager.get_status()
for provider in status['llm_providers']:
    if provider['success_count'] > 0:
        print(f"‚úÖ {provider['name']}: {provider['success_count']} requests")
```

## üöÄ Vantagens

1. **Resili√™ncia**: Nunca para por falha de um provedor
2. **Economia**: Usa provedores gratuitos primeiro
3. **Flexibilidade**: F√°cil adicionar novos provedores
4. **Transpar√™ncia**: Logs detalhados de cada tentativa
5. **Performance**: Load balancing opcional para distribuir carga

## üìä Monitoramento

O sistema rastreia automaticamente:
- N√∫mero de requisi√ß√µes bem-sucedidas por provedor
- N√∫mero de falhas consecutivas
- √öltimo erro registrado
- Status atual (ativo, falhou, rate-limited, desabilitado)

## üîÑ Pr√≥ximos Provedores

F√°cil adicionar novos provedores implementando a interface `LLMProvider`:

```python
class NovoProvider(LLMProvider):
    @property
    def name(self) -> str:
        return "novo_provider"

    async def generate_text(self, prompt: str, **kwargs) -> str:
        # Implementa√ß√£o
        pass

    async def generate_json(self, prompt: str, **kwargs) -> dict:
        # Implementa√ß√£o
        pass
```
